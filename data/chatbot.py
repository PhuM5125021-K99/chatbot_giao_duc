import os
from langchain_community.llms import Ollama
from langchain_core.prompts import PromptTemplate
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_community.cache import InMemoryCache
import langchain


# ====== Cáº¥u hÃ¬nh cache (tÄƒng tá»‘c LLM) ======
langchain.llm_cache = InMemoryCache()


# ====== ThÃ´ng tin há»‡ thá»‘ng ======
DATA_PATH = "kien_thuc_giao_duc.txt"
CHROMA_DIR = "data/chroma_db"
OLLAMA_BASE = "http://localhost:11434"

EMBED_MODEL = "nomic-embed-text"
LLM_MODEL = "llama3.1:8b"   # cÃ³ thá»ƒ Ä‘á»•i sang qwen2.5:7b hoáº·c llama3.1:8b cho nhanh hÆ¡n


# ====== 1) Load dá»¯ liá»‡u ======
if not os.path.exists(DATA_PATH):
    raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u: {DATA_PATH}")

print("ğŸ“˜ Äang táº£i dá»¯ liá»‡u...")
loader = TextLoader(DATA_PATH, encoding="utf-8")
documents = loader.load()


# ====== 2) Chia nhá» vÄƒn báº£n ======
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = splitter.split_documents(documents)
print(f"âœ… ÄÃ£ chia thÃ nh {len(chunks)} Ä‘oáº¡n.")


# ====== 3) Embedding + Vectorstore ======
print("ğŸ”¢ Äang táº¡o embeddings...")
embeddings = OllamaEmbeddings(model=EMBED_MODEL, base_url=OLLAMA_BASE)

vectorstore = Chroma.from_documents(
    chunks,
    embedding=embeddings,
    persist_directory=CHROMA_DIR
)
vectorstore.persist()
print("ğŸ’¾ Vectorstore Ä‘Ã£ sáºµn sÃ ng.")


# ====== 4) Khá»Ÿi táº¡o LLM ======
llm = Ollama(model=LLM_MODEL, base_url=OLLAMA_BASE)


# ====== 5) Prompt rÃºt gá»n & tá»‘i Æ°u tá»‘c Ä‘á»™ ======
EDU_PROMPT = """
Báº¡n lÃ  "Trá»£ lÃ½ Há»c vá»¥ CTU" â€” trá»£ lÃ½ áº£o chÃ­nh thá»©c cho sinh viÃªn, giáº£ng viÃªn vÃ  phá»¥ huynh cá»§a TrÆ°á»ng Äáº¡i há»c Cáº§n ThÆ¡ (CTU).

Má»¤C TIÃŠU:
- Tráº£ lá»i chÃ­nh xÃ¡c, ngáº¯n gá»n, báº±ng tiáº¿ng Viá»‡t.
- Náº¿u cÃ¢u há»i liÃªn quan Ä‘áº¿n thao tÃ¡c (Ä‘Äƒng kÃ½ mÃ´n, tra cá»©u Ä‘iá»ƒm, xem lá»‹ch thi, lá»‹ch há»c), luÃ´n cung cáº¥p link chÃ­nh thá»©c.
- Æ¯u tiÃªn cÃ¢u tráº£ lá»i dá»±a trÃªn dá»¯ liá»‡u RAG (context) â†’ khÃ´ng bá»‹a khi thiáº¿u thÃ´ng tin.

QUY Táº®C:
1. KhÃ´ng Ä‘Æ°á»£c táº¡o thÃ´ng tin náº¿u khÃ´ng cháº¯c cháº¯n. Náº¿u khÃ´ng cÃ³ trong dá»¯ liá»‡u RAG, hÃ£y nÃ³i: 
   â€œDá»¯ liá»‡u nÃ y khÃ´ng cÃ³ trong nguá»“n hiá»‡n cÃ³. Báº¡n cÃ³ thá»ƒ xem táº¡i <link chÃ­nh thá»©c> hoáº·c liÃªn há»‡ phÃ²ng ban.â€
2. Náº¿u sá»­ dá»¥ng dá»¯ liá»‡u tá»« RAG, pháº£i thÃªm: [Nguá»“n: <file hoáº·c link>].
3. Tráº£ lá»i theo format:
   - 1â€“2 cÃ¢u ngáº¯n tÃ³m táº¯t
   - Náº¿u lÃ  thao tÃ¡c: liá»‡t kÃª bÆ°á»›c 1â€“2â€“3 + link chÃ­nh thá»©c
   - DÃ²ng cuá»‘i: [Nguá»“n: ...] + [confidence: XX%]
4. KhÃ´ng xá»­ lÃ½ thÃ´ng tin cÃ¡ nhÃ¢n (Ä‘iá»ƒm riÃªng, MSSV). Chá»‰ hÆ°á»›ng dáº«n truy cáº­p há»‡ thá»‘ng.
5. KhÃ´ng lan man, khÃ´ng dÃ i dÃ²ng, khÃ´ng nÃ³i mÆ¡ há»“.
6. LuÃ´n Æ°u tiÃªn cÃ¡c link chÃ­nh thá»©c:
   - ÄÄƒng kÃ½ há»c pháº§n: https://dkmhfe.ctu.edu.vn
   - Tra cá»©u Ä‘iá»ƒm/lá»‹ch thi/lá»‹ch há»c: https://htql.ctu.edu.vn
   - Há»— trá»£ ká»¹ thuáº­t: https://helpdesk.ctu.edu.vn
   - PhÃ²ng ÄÃ o táº¡o: pdt@ctu.edu.vn â€” 0292 383 1156
   - PhÃ²ng CTSV: pctsv@ctu.edu.vn â€” 0292 387 2177

OUTPUT:
- Tráº£ lá»i ngáº¯n gá»n (3â€“6 cÃ¢u), cÃ³ nguá»“n, cÃ³ confidence.
"""

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=EDU_PROMPT + "\n\nThÃ´ng tin tham kháº£o:\n{context}\n\nCÃ¢u há»i: {question}\nTráº£ lá»i ngáº¯n gá»n:"
)


# ====== 6) VÃ²ng láº·p giao tiáº¿p ======
print("\nğŸ“ Chatbot CTU Ä‘Ã£ sáºµn sÃ ng! (gÃµ 'exit' Ä‘á»ƒ thoÃ¡t)\n")

while True:
    q = input("ğŸ‘©â€ğŸ“ Báº¡n: ").strip()
    if q.lower() == "exit":
        print("ğŸ‘‹ Táº¡m biá»‡t! ChÃºc báº¡n má»™t ngÃ y tá»‘t lÃ nh!")
        break

    try:
        # Láº¥y context nhanh (tá»‘i Æ°u k=2)
        results = vectorstore.similarity_search(q, k=2)
        context = "\n\n".join([doc.page_content for doc in results])

        # Táº¡o prompt cuá»‘i
        final_prompt = prompt.format(context=context, question=q)

        # Gá»i LLM vá»›i temperature tháº¥p (tráº£ lá»i nhanh & Ã­t suy nghÄ©)
        print("ğŸ¤– Trá»£ lÃ½ CTU: ", end="", flush=True)

        for chunk in llm.stream(final_prompt):
            print(chunk, end="", flush=True)

        print("\n")  # xuá»‘ng dÃ²ng sau khi stream xong

    except Exception as e:
        print(f"âš ï¸ Lá»—i: {e}\n")
